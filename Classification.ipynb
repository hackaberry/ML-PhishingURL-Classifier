{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce25957-aed6-4e2a-bf02-87cb762098a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from DataPrep import X_train, X_test, y_train, y_test # Previous notebook with data now processed\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f88e843-5fd8-407a-9703-fcf5f3b64c44",
   "metadata": {},
   "source": [
    "## Further preparing data & setting a batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49b744-1f85-494e-8605-11696cd61430",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "in_features = 30\n",
    "out_features = 1\n",
    "hidden_units = 5 # baseline hidden_units\n",
    "hidden_units_v2 = 10 # For testing if more neurons = better\n",
    "\n",
    "learning_rate = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Mapping values: -1 -> 0, and 1 -> 2 on all dataset labels because BCE doesn't like negative numbers\n",
    "y_train_mapped, y_test_mapped = y_train.clone().long(), y_test.clone().long()\n",
    "mapped_values = {-1:0, 1:1}\n",
    "\n",
    "y_train_mapped.apply_(lambda x: mapped_values[x])\n",
    "y_test_mapped.apply_(lambda x: mapped_values[x])\n",
    "\n",
    "# Get dataset ready for DataLoader\n",
    "train_dataset = TensorDataset(X_train,y_train_mapped)\n",
    "test_dataset = TensorDataset(X_test,y_test_mapped)\n",
    "\n",
    "# Mini-batching data\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ccd02-a757-4a92-806a-72a2b63a9fea",
   "metadata": {},
   "source": [
    "## ModelV0: 3 layers deep, 5 hidden neurons. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b32498-b4b5-4881-bddc-63557ea3c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhishingModelV0(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, hidden_units):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features = in_features , out_features = hidden_units)\n",
    "        self.layer_2 = nn.Linear(in_features = hidden_units, out_features = hidden_units)\n",
    "        self.layer_3 = nn.Linear(in_features = hidden_units, out_features = out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "        z = self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(X)))))\n",
    "        return z\n",
    "\n",
    "modelV0 = PhishingModelV0(in_features = in_features, out_features = out_features, hidden_units = hidden_units).to(device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5855c-c317-4ccf-8135-5bd173cff740",
   "metadata": {},
   "source": [
    "## ModelV1: 6 layers deep, 5 hidden neurons. Is deeper better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7833406-bfbc-457e-98ed-ab0232169869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhishingModelV1(nn.Module): # I know sequential is easier to read in this case. But wanted to stay consistent with previous model\n",
    "\n",
    "    def __init__(self, in_features, out_features, hidden_units):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features = in_features , out_features = hidden_units)\n",
    "        self.layer_2 = nn.Linear(in_features = hidden_units, out_features = hidden_units)\n",
    "        self.layer_3 = nn.Linear(in_features = hidden_units, out_features = hidden_units)\n",
    "        self.layer_4 = nn.Linear(in_features = hidden_units, out_features = hidden_units)\n",
    "        self.layer_5 = nn.Linear(in_features = hidden_units, out_features = hidden_units)\n",
    "        self.layer_6 = nn.Linear(in_features = hidden_units, out_features = out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "        z = self.layer_6(self.relu(self.layer_5(self.relu(self.layer_4(self.relu(self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(X)))))))))))\n",
    "        return z\n",
    "modelV1 = PhishingModelV1(in_features = in_features, out_features = out_features, hidden_units = hidden_units).to(device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56960101-f0b9-4ff1-b2f2-27f163bd4167",
   "metadata": {},
   "source": [
    "## ModelV2: 3 layers deep, 10 hidden_neurons. Is wider better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0401ac2-ea04-499b-943b-9dc80c09a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelV2 = PhishingModelV0(in_features = in_features, out_features = out_features, hidden_units = hidden_units_v2).to(device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b085c141-0801-4f25-9c35-bfbf9ede79ca",
   "metadata": {},
   "source": [
    "## ModelV3: 6 layers deep, 10 hidden_neurons. Are both deep & wide better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d670b16-c6fa-41a3-acd2-9f74154fd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelV3 = PhishingModelV1(in_features = in_features, out_features = out_features, hidden_units = hidden_units_v2).to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947176c0-61a0-4242-a5dd-8287e5c18319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_preds, y_true):\n",
    "    correct = torch.eq(y_preds, y_true).sum().item()\n",
    "    acc = correct / int(len(y_preds)) * 100\n",
    "    return round(acc,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55380e09-e5fe-486e-83e2-8e31dfa7fad4",
   "metadata": {},
   "source": [
    "## Training & Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96647243-b66d-44bf-92a5-75b9ec008a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, model_name, epochs=101):\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr = learning_rate)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "    \n",
    "        for X_batch_train, y_batch_train in train_loader:\n",
    "            train_logits = model(X_batch_train).squeeze()\n",
    "            train_labels = torch.sigmoid(train_logits).round()\n",
    "            train_accuracy = accuracy_fn(train_labels,y_batch_train)\n",
    "    \n",
    "            train_loss = loss_fn(train_logits,y_batch_train.type(torch.float))\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            for X_batch_test, y_batch_test in test_loader: \n",
    "                with torch.inference_mode():\n",
    "                    test_logits = model(X_batch_test).squeeze()\n",
    "                    test_labels = torch.sigmoid(test_logits).round()\n",
    "                    test_accuracy = accuracy_fn(test_labels,y_batch_test)\n",
    "    \n",
    "                    test_loss = loss_fn(test_logits,y_batch_test.type(torch.float))\n",
    "                    all_preds.append(test_labels)\n",
    "                    all_labels.append(y_batch_test)\n",
    "                \n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            test_accuracy = accuracy_fn(all_preds,all_labels)\n",
    "            \n",
    "            print(f\"Epoch: {epoch} | Test Accuracy: {test_accuracy}% | Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63e03e-056a-4ae0-b4e3-e2490e99c7a7",
   "metadata": {},
   "source": [
    "## Training & saving the best of 5 of each: (commented out to avoid rerunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddb2e7-2644-41bb-ab7d-f290be17676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loop(modelV0,\"ModelV0\") # 3 layers, 5 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a8afc-37f8-4c33-ac2c-2e7403660619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loop(modelV1,\"ModelV1\") # 6 layers, 5 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e972c-0a6a-4f4e-a521-25de7c1146e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loop(modelV2,\"ModelV2\") # 3 layers, 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430b20d-bc42-44d7-bb75-d6e99e0e666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loop(modelV3,\"ModelV3\") # 6 layers and 10 neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61800056-50bd-41b2-89e8-d404ac6d1049",
   "metadata": {},
   "source": [
    "### Saving all 4 models to evaluate their metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a8f9c-220e-4793-983a-879e3a943fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(obj=modelV0.state_dict(), f=\"saved_models/modelV0.pt\")\n",
    "# torch.save(obj=modelV1.state_dict(), f=\"saved_models/modelV1.pt\")\n",
    "# torch.save(obj=modelV2.state_dict(), f=\"saved_models/modelV2.pt\")\n",
    "# torch.save(obj=modelV3.state_dict(), f=\"saved_models/modelV3.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
